# Simple RAG (Retrieval-Augmented Generation) Demo

This project is a **simple, end-to-end Retrieval-Augmented Generation (RAG) application** built to demonstrate how large language models can answer questions from custom documents.

The app allows users to **upload a PDF**, ask questions about its content, and receive answers generated using **retrieved document context**.

The project is intentionally kept **simple, lightweight, and free** so it can be easily deployed on **Streamlit Community Cloud** and explained clearly in interviews.

---

## üöÄ Features

- Upload any PDF document
- Automatic text chunking and embedding
- Semantic search using FAISS
- Context-aware answer generation
- No paid APIs
- CPU-only (cloud friendly)
- Deployed using Streamlit

---

## üß† How RAG Works in This Project

This project follows a **classic RAG pipeline**:

### 1Ô∏è‚É£ Document Loading

- The uploaded PDF is loaded using `PyPDFLoader`.

### 2Ô∏è‚É£ Text Splitting

- The document is split into small overlapping chunks using `RecursiveCharacterTextSplitter`.
- This improves retrieval accuracy.

### 3Ô∏è‚É£ Embedding Generation

- Each chunk is converted into a vector embedding using:

### 4Ô∏è‚É£ Vector Storage (FAISS)

- All embeddings are stored in a FAISS vector database for fast similarity search.

### 5Ô∏è‚É£ Retrieval

- When a user asks a question, the most relevant document chunks are retrieved from FAISS.

### 6Ô∏è‚É£ Answer Generation

- The retrieved context is combined with the user‚Äôs question.
- A lightweight open-source language model (`google/flan-t5-small`) generates the final answer.

This approach ensures the model answers **based on the document**, not hallucinations.

---

## üèóÔ∏è Project Structure

### 4Ô∏è‚É£ Vector Storage (FAISS)

- All embeddings are stored in a FAISS vector database for fast similarity search.

### 5Ô∏è‚É£ Retrieval

- When a user asks a question, the most relevant document chunks are retrieved from FAISS.

### 6Ô∏è‚É£ Answer Generation

- The retrieved context is combined with the user‚Äôs question.
- A lightweight open-source language model (`google/flan-t5-small`) generates the final answer.

This approach ensures the model answers **based on the document**, not hallucinations.

## üñ•Ô∏è Tech Stack

- **Python 3.10**
- **Streamlit** ‚Äì UI and hosting
- **LangChain (community modules)** ‚Äì document loading & embeddings
- **FAISS** ‚Äì vector database
- **Sentence Transformers** ‚Äì text embeddings
- **Hugging Face Transformers** ‚Äì lightweight open-source LLM
- **PyPDF** ‚Äì PDF parsing

---
